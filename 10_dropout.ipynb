{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ccd702",
   "metadata": {},
   "source": [
    "# №10. Борьба с переобучением. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a6d21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. https://habr.com/ru/companies/mvideo/articles/782360\n",
    "2. https://habr.com/ru/companies/wunderfund/articles/330814\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55baa4db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Краткая теория"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ec75f",
   "metadata": {},
   "source": [
    "*Схема работы метода **Dropout**:*\n",
    "\n",
    "<img src=\"data/my_images/dropout_schema.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd0050",
   "metadata": {},
   "source": [
    "Главная идея Dropout — вместо обучения одной DNN обучить ансамбль нескольких DNN, а затем усреднить полученные результаты.\n",
    "\n",
    "Сети для обучения получаются с помощью исключения из сети (dropping out) нейронов с вероятностью $p$, таким образом, вероятность того, что нейрон останется в сети, составляет $q=1-p$. “Исключение” нейрона означает, что при любых входных данных или параметрах он возвращает 0.\n",
    "\n",
    "Исключенные нейроны не вносят свой вклад в процесс обучения ни на одном из этапов алгоритма обратного распространения ошибки (backpropagation); поэтому исключение хотя бы одного из нейронов равносильно обучению новой нейронной сети.\n",
    "\n",
    "[Цитируя авторов](https://arxiv.org/abs/1207.0580),\n",
    "\n",
    "> В стандартной нейронной сети производная, полученная каждым параметром, сообщает ему, как он должен измениться, чтобы, учитывая деятельность остальных блоков, минимизировать функцию конечных потерь. Поэтому блоки могут меняться, исправляя при этом ошибки других блоков. Это может привести к чрезмерной совместной адаптации (co-adaptation), что, в свою очередь, приводит к переобучению, поскольку эти совместные адаптации невозможно обобщить на данные, не участвовавшие в обучении. Мы выдвигаем гипотезу, что Dropout предотвращает совместную адаптацию для каждого скрытого блока, делая присутствие других скрытых блоков ненадежным. Поэтому скрытый блок не может полагаться на другие блоки в исправлении собственных ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c699f",
   "metadata": {},
   "source": [
    "## 2. Использование в `tocrh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0504de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab5f9c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.]]) tensor(4.5000)\n",
      "out:  tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]) tensor(0.)\n",
      "mean:  tensor(4.5037) tensor(4.5000)\n"
     ]
    }
   ],
   "source": [
    "# Создание слоя dropout\n",
    "from sympy import N\n",
    "\n",
    "p = 0.5\n",
    "dropout = nn.Dropout(p=p)  # вероятность отключения p\n",
    "\n",
    "# Применение к тензору\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
    "                  [5.0, 6.0, 7.0, 8.0]])\n",
    "print(\"x: \", x, x.mean())\n",
    "output = dropout(x)\n",
    "print(\"out: \", output, output.mean())\n",
    "\n",
    "n=1000\n",
    "mean_list = []\n",
    "for i in range(n):\n",
    "    output = dropout(x)\n",
    "    mean_list.append(output.mean())\n",
    "    \n",
    "print(\"mean: \", sum(mean_list) / n, x.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9e592",
   "metadata": {},
   "source": [
    "**Примечание**: ***dropout*** учсаствует в процессе обучения (`model.train()`), однако при использовании модели (или ее тестировании, валидации), необходимо перевести модель в режим работы (`model.eval()`), так как нам необходимы все нейроны для работы сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRegModel(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__()\n",
    "        hidden_size = 5\n",
    "        self.layer_1 = nn.Linear(input, hidden_size)\n",
    "        self.dp = nn.Dropout(p=0.5)\n",
    "        self.layer_2 = nn.Linear(hidden_size, output)\n",
    "        self.act_func = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(f\"input: \", x)\n",
    "        x = self.layer_1(x)\n",
    "        print(f\"after layer_1: \", x)\n",
    "        x = self.dp(x)\n",
    "        print(f\"after_dropout: \", x)\n",
    "        x = self.act_func(x)\n",
    "        print(f\"after ReLU: \", x)\n",
    "        out = self.layer_2(x)\n",
    "        print(f\"after layer_2 (out): \", out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d756a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 10])\n",
    "reg_model = myRegModel(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "954ac1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[0.8625, 0.3690, 0.2415, 0.4812, 0.3050, 0.6386, 0.3207, 0.4484, 0.8921,\n",
      "         0.1570]])\n",
      "after layer_1:  tensor([[ 0.0272, -0.5179,  0.0742, -0.4964, -0.5511]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "after_dropout:  tensor([[ 0.0272, -0.5179,  0.0742, -0.4964, -0.5511]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "after ReLU:  tensor([[0.0272, 0.0000, 0.0742, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "after layer_2 (out):  tensor([[-0.1911,  0.3191]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1911,  0.3191]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model.eval()\n",
    "reg_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8073d6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[0.8625, 0.3690, 0.2415, 0.4812, 0.3050, 0.6386, 0.3207, 0.4484, 0.8921,\n",
      "         0.1570]])\n",
      "after layer_1:  tensor([[ 0.0272, -0.5179,  0.0742, -0.4964, -0.5511]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "after_dropout:  tensor([[ 0.0000, -0.0000,  0.0000, -0.9927, -1.1022]], grad_fn=<MulBackward0>)\n",
      "after ReLU:  tensor([[0., -0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "after layer_2 (out):  tensor([[-0.2121,  0.2937]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2121,  0.2937]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model.train()\n",
    "reg_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db78a57",
   "metadata": {},
   "source": [
    "## 3. Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343ef78",
   "metadata": {},
   "source": [
    "Dropout существует в двух модификациях: прямой (используется нечасто) и обратный.\n",
    "\n",
    "Dropout на отдельном нейроне может быть представлен как случайная величина с распределением Бернулли.\n",
    "\n",
    "Dropout на множестве нейронов может быть представлен как случайная величина с биномиальным распределением.\n",
    "\n",
    "Несмотря на то, что вероятность того, что из сети будет выключено ровно np нейронов, np — среднее количество нейронов, отключенных в слое из n нейронов.\n",
    "\n",
    "Обратный Dropout увеличивает скорость обучения.\n",
    "\n",
    "Обратный Dropout следует использовать совместно с другими методами нормализации, ограничивающими значения параметров, чтобы упростить процесс выбора скорости обучения.\n",
    "\n",
    "Dropout помогает предотвратить проблему обучения в глубоких нейронных сетях."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ML-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
